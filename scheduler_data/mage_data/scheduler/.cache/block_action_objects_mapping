{"block_file": {"data_exporters/extractor_qb_customers.py:data_exporter:python:extractor qb customers": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom datetime import datetime\nfrom os import path\nimport psycopg2\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport json\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    if df is None or df.empty:\n        print(\"No hay datos para exportar.\")\n        return\n\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    dbname = get_secret_value('pg_db')\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n\n    schema = 'raw'\n    table = 'qb_items'\n    report_table = 'backfill_report_items'\n    pipeline_name = \"qb_items_backfill\"\n\n    # Serializar payloads\n    df = df.copy()\n    if \"payload\" in df.columns:\n        df[\"payload\"] = df[\"payload\"].apply(\n            lambda x: json.dumps(x) if isinstance(x, (dict, list)) else str(x)\n        )\n    if \"request_payload\" in df.columns:\n        df[\"request_payload\"] = df[\"request_payload\"].apply(\n            lambda x: json.dumps(x) if isinstance(x, (dict, list)) else str(x)\n        )\n\n    started_at = datetime.utcnow()\n    print(f\"Exportando {len(df)} filas a {schema}.{table} en {host}:{port}/{dbname}\")\n\n    conn = psycopg2.connect(\n        host=host,\n        port=port,\n        dbname=dbname,\n        user=user,\n        password=password\n    )\n    conn.autocommit = False\n\n    try:\n        with conn.cursor() as cur:\n            # Crear esquema\n            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n\n            # Crear tabla destino\n            ddl = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema}.{table} (\n                id TEXT PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP,\n                extract_window_start_utc TIMESTAMP,\n                extract_window_end_utc TIMESTAMP,\n                page_number INT,\n                page_size INT,\n                request_payload TEXT\n            );\n            \"\"\"\n            cur.execute(ddl)\n\n            # Insert con UPSERT\n            insert_sql = f\"\"\"\n            INSERT INTO {schema}.{table} (\n                id, payload, ingested_at_utc,\n                extract_window_start_utc, extract_window_end_utc,\n                page_number, page_size, request_payload\n            )\n            VALUES (\n                %(id)s, %(payload)s, %(ingested_at_utc)s,\n                %(extract_window_start_utc)s, %(extract_window_end_utc)s,\n                %(page_number)s, %(page_size)s, %(request_payload)s\n            )\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number,\n                page_size = EXCLUDED.page_size,\n                request_payload = EXCLUDED.request_payload;\n            \"\"\"\n            records = df.to_dict(orient='records')\n            for row in records:\n                cur.execute(insert_sql, row)\n\n            # Crear tabla de reporte\n            ddl_report = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema}.{report_table} (\n                id SERIAL PRIMARY KEY,\n                pipeline_name TEXT,\n                entity TEXT,\n                row_count INT,\n                window_start TIMESTAMP,\n                window_end TIMESTAMP,\n                started_at TIMESTAMP,\n                ended_at TIMESTAMP,\n                status TEXT,\n                error_message TEXT\n            );\n            \"\"\"\n            cur.execute(ddl_report)\n\n            # Insertar reporte de \u00e9xito\n            ended_at = datetime.utcnow()\n            cur.execute(\n                f\"\"\"\n                INSERT INTO {schema}.{report_table} \n                (pipeline_name, entity, row_count, window_start, window_end, \n                 started_at, ended_at, status, error_message)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n                \"\"\",\n                (\n                    pipeline_name,\n                    \"items\",\n                    len(df),\n                    df[\"extract_window_start_utc\"].min(),\n                    df[\"extract_window_end_utc\"].max(),\n                    started_at,\n                    ended_at,\n                    \"success\",\n                    None,\n                )\n            )\n\n        conn.commit()\n        print(f\"Exportaci\u00f3n completada: {len(df)} filas procesadas.\")\n\n    except Exception as e:\n        conn.rollback()\n        ended_at = datetime.utcnow()\n        with conn.cursor() as cur:\n            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n            cur.execute(f\"\"\"\n                CREATE TABLE IF NOT EXISTS {schema}.{report_table} (\n                    id SERIAL PRIMARY KEY,\n                    pipeline_name TEXT,\n                    entity TEXT,\n                    row_count INT,\n                    window_start TIMESTAMP,\n                    window_end TIMESTAMP,\n                    started_at TIMESTAMP,\n                    ended_at TIMESTAMP,\n                    status TEXT,\n                    error_message TEXT\n                );\n            \"\"\")\n            # Reporte de error\n            cur.execute(\n                f\"\"\"\n                INSERT INTO {schema}.{report_table} \n                (pipeline_name, entity, row_count, window_start, window_end, \n                 started_at, ended_at, status, error_message)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n                \"\"\",\n                (\n                    pipeline_name,\n                    \"items\",\n                    0,\n                    None,\n                    None,\n                    started_at,\n                    ended_at,\n                    \"error\",\n                    str(e),\n                )\n            )\n        conn.commit()\n        print(f\"Error al exportar datos: {e}\")\n        raise e\n    finally:\n        conn.close()", "file_path": "data_exporters/extractor_qb_customers.py", "language": "python", "type": "data_exporter", "uuid": "extractor_qb_customers"}, "data_exporters/extractor_qb_items.py:data_exporter:python:extractor qb items": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom datetime import datetime\nfrom os import path\nimport psycopg2\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport json\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    if df is None or df.empty:\n        print(\"No hay datos para exportar.\")\n        return\n\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    dbname = get_secret_value('pg_db')\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n\n    schema = 'raw'\n    table = 'qb_items'\n    report_table = 'backfill_report_items'\n    pipeline_name = \"qb_items_backfill\"\n\n    # Serializar payloads\n    df = df.copy()\n    if \"payload\" in df.columns:\n        df[\"payload\"] = df[\"payload\"].apply(\n            lambda x: json.dumps(x) if isinstance(x, (dict, list)) else str(x)\n        )\n    if \"request_payload\" in df.columns:\n        df[\"request_payload\"] = df[\"request_payload\"].apply(\n            lambda x: json.dumps(x) if isinstance(x, (dict, list)) else str(x)\n        )\n\n    started_at = datetime.utcnow()\n    print(f\"Exportando {len(df)} filas a {schema}.{table} en {host}:{port}/{dbname}\")\n\n    conn = psycopg2.connect(\n        host=host,\n        port=port,\n        dbname=dbname,\n        user=user,\n        password=password\n    )\n    conn.autocommit = False\n\n    try:\n        with conn.cursor() as cur:\n            # Crear esquema\n            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n\n            # Crear tabla destino\n            ddl = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema}.{table} (\n                id TEXT PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP,\n                extract_window_start_utc TIMESTAMP,\n                extract_window_end_utc TIMESTAMP,\n                page_number INT,\n                page_size INT,\n                request_payload TEXT\n            );\n            \"\"\"\n            cur.execute(ddl)\n\n            # Insert con UPSERT\n            insert_sql = f\"\"\"\n            INSERT INTO {schema}.{table} (\n                id, payload, ingested_at_utc,\n                extract_window_start_utc, extract_window_end_utc,\n                page_number, page_size, request_payload\n            )\n            VALUES (\n                %(id)s, %(payload)s, %(ingested_at_utc)s,\n                %(extract_window_start_utc)s, %(extract_window_end_utc)s,\n                %(page_number)s, %(page_size)s, %(request_payload)s\n            )\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number,\n                page_size = EXCLUDED.page_size,\n                request_payload = EXCLUDED.request_payload;\n            \"\"\"\n            records = df.to_dict(orient='records')\n            for row in records:\n                cur.execute(insert_sql, row)\n\n            # Crear tabla de reporte\n            ddl_report = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema}.{report_table} (\n                id SERIAL PRIMARY KEY,\n                pipeline_name TEXT,\n                entity TEXT,\n                row_count INT,\n                window_start TIMESTAMP,\n                window_end TIMESTAMP,\n                started_at TIMESTAMP,\n                ended_at TIMESTAMP,\n                status TEXT,\n                error_message TEXT\n            );\n            \"\"\"\n            cur.execute(ddl_report)\n\n            # Insertar reporte de \u00e9xito\n            ended_at = datetime.utcnow()\n            cur.execute(\n                f\"\"\"\n                INSERT INTO {schema}.{report_table} \n                (pipeline_name, entity, row_count, window_start, window_end, \n                 started_at, ended_at, status, error_message)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n                \"\"\",\n                (\n                    pipeline_name,\n                    \"items\",\n                    len(df),\n                    df[\"extract_window_start_utc\"].min(),\n                    df[\"extract_window_end_utc\"].max(),\n                    started_at,\n                    ended_at,\n                    \"success\",\n                    None,\n                )\n            )\n\n        conn.commit()\n        print(f\"Exportaci\u00f3n completada: {len(df)} filas procesadas.\")\n\n    except Exception as e:\n        conn.rollback()\n        ended_at = datetime.utcnow()\n        with conn.cursor() as cur:\n            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n            cur.execute(f\"\"\"\n                CREATE TABLE IF NOT EXISTS {schema}.{report_table} (\n                    id SERIAL PRIMARY KEY,\n                    pipeline_name TEXT,\n                    entity TEXT,\n                    row_count INT,\n                    window_start TIMESTAMP,\n                    window_end TIMESTAMP,\n                    started_at TIMESTAMP,\n                    ended_at TIMESTAMP,\n                    status TEXT,\n                    error_message TEXT\n                );\n            \"\"\")\n            # Reporte de error\n            cur.execute(\n                f\"\"\"\n                INSERT INTO {schema}.{report_table} \n                (pipeline_name, entity, row_count, window_start, window_end, \n                 started_at, ended_at, status, error_message)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n                \"\"\",\n                (\n                    pipeline_name,\n                    \"items\",\n                    0,\n                    None,\n                    None,\n                    started_at,\n                    ended_at,\n                    \"error\",\n                    str(e),\n                )\n            )\n        conn.commit()\n        print(f\"Error al exportar datos: {e}\")\n        raise e\n    finally:\n        conn.close()", "file_path": "data_exporters/extractor_qb_items.py", "language": "python", "type": "data_exporter", "uuid": "extractor_qb_items"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/extractor_qb_invoices.py:data_exporter:python:extractor qb invoices": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom datetime import datetime\nfrom os import path\nimport psycopg2\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport json\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    if df is None or df.empty:\n        print(\"No hay datos para exportar.\")\n        return\n\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    dbname = get_secret_value('pg_db')\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n\n    schema = 'raw'\n    table = 'qb_invoices'\n    report_table = 'backfill_report_invoices'\n    pipeline_name = \"qb_invoices_backfill\"\n\n    # Serializar payloads\n    df = df.copy()\n    if \"payload\" in df.columns:\n        df[\"payload\"] = df[\"payload\"].apply(\n            lambda x: json.dumps(x) if isinstance(x, (dict, list)) else str(x)\n        )\n    if \"request_payload\" in df.columns:\n        df[\"request_payload\"] = df[\"request_payload\"].apply(\n            lambda x: json.dumps(x) if isinstance(x, (dict, list)) else str(x)\n        )\n\n    started_at = datetime.utcnow()\n    print(f\"Exportando {len(df)} filas a {schema}.{table} en {host}:{port}/{dbname}\")\n\n    conn = psycopg2.connect(\n        host=host,\n        port=port,\n        dbname=dbname,\n        user=user,\n        password=password\n    )\n    conn.autocommit = False\n\n    try:\n        with conn.cursor() as cur:\n            # Crear esquema\n            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n\n            # Crear tabla destino\n            ddl = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema}.{table} (\n                id TEXT PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP,\n                extract_window_start_utc TIMESTAMP,\n                extract_window_end_utc TIMESTAMP,\n                page_number INT,\n                page_size INT,\n                request_payload TEXT\n            );\n            \"\"\"\n            cur.execute(ddl)\n\n            # Insert con UPSERT\n            insert_sql = f\"\"\"\n            INSERT INTO {schema}.{table} (\n                id, payload, ingested_at_utc,\n                extract_window_start_utc, extract_window_end_utc,\n                page_number, page_size, request_payload\n            )\n            VALUES (\n                %(id)s, %(payload)s, %(ingested_at_utc)s,\n                %(extract_window_start_utc)s, %(extract_window_end_utc)s,\n                %(page_number)s, %(page_size)s, %(request_payload)s\n            )\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number,\n                page_size = EXCLUDED.page_size,\n                request_payload = EXCLUDED.request_payload;\n            \"\"\"\n            records = df.to_dict(orient='records')\n            for row in records:\n                cur.execute(insert_sql, row)\n\n            # Crear tabla de reporte\n            ddl_report = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema}.{report_table} (\n                id SERIAL PRIMARY KEY,\n                pipeline_name TEXT,\n                entity TEXT,\n                row_count INT,\n                window_start TIMESTAMP,\n                window_end TIMESTAMP,\n                started_at TIMESTAMP,\n                ended_at TIMESTAMP,\n                status TEXT,\n                error_message TEXT\n            );\n            \"\"\"\n            cur.execute(ddl_report)\n\n            # Insertar reporte de \u00e9xito\n            ended_at = datetime.utcnow()\n            cur.execute(\n                f\"\"\"\n                INSERT INTO {schema}.{report_table} \n                (pipeline_name, entity, row_count, window_start, window_end, \n                 started_at, ended_at, status, error_message)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n                \"\"\",\n                (\n                    pipeline_name,\n                    \"invoices\",\n                    len(df),\n                    df[\"extract_window_start_utc\"].min(),\n                    df[\"extract_window_end_utc\"].max(),\n                    started_at,\n                    ended_at,\n                    \"success\",\n                    None,\n                )\n            )\n\n        conn.commit()\n        print(f\"Exportaci\u00f3n completada: {len(df)} filas procesadas.\")\n\n    except Exception as e:\n        conn.rollback()\n        ended_at = datetime.utcnow()\n        with conn.cursor() as cur:\n            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n            cur.execute(f\"\"\"\n                CREATE TABLE IF NOT EXISTS {schema}.{report_table} (\n                    id SERIAL PRIMARY KEY,\n                    pipeline_name TEXT,\n                    entity TEXT,\n                    row_count INT,\n                    window_start TIMESTAMP,\n                    window_end TIMESTAMP,\n                    started_at TIMESTAMP,\n                    ended_at TIMESTAMP,\n                    status TEXT,\n                    error_message TEXT\n                );\n            \"\"\")\n            # Reporte de error\n            cur.execute(\n                f\"\"\"\n                INSERT INTO {schema}.{report_table} \n                (pipeline_name, entity, row_count, window_start, window_end, \n                 started_at, ended_at, status, error_message)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n                \"\"\",\n                (\n                    pipeline_name,\n                    \"invoices\",\n                    0,\n                    None,\n                    None,\n                    started_at,\n                    ended_at,\n                    \"error\",\n                    str(e),\n                )\n            )\n        conn.commit()\n        print(f\"Error al exportar datos: {e}\")\n        raise e\n    finally:\n        conn.close()", "file_path": "data_exporters/extractor_qb_invoices.py", "language": "python", "type": "data_exporter", "uuid": "extractor_qb_invoices"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/creative_shape.py:data_loader:python:creative shape": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your data loading logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/creative_shape.py", "language": "python", "type": "data_loader", "uuid": "creative_shape"}, "data_loaders/ingest_qb_customers.py:data_loader:python:ingest qb customers": {"content": "import time\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef _refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_secret_id')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n    }\n\n    respuesta = requests.post(url, headers=headers, data=data, auth=(client_id, client_secret))\n    respuesta.raise_for_status()\n    tokens = respuesta.json()\n    print(\"Nuevo access_token obtenido v\u00eda refresh_token\")\n    return tokens[\"access_token\"]\n\n\ndef _fetch_qb_data(realm_id, access_token, base_url, minor_version,\n                   fecha_inicio, fecha_fin, max_retries=5):\n    if not realm_id or not access_token:\n        raise ValueError(\"Se requieren realm_id y access_token\")\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere base_url y minor_version\")\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Se requieren fecha_inicio y fecha_fin\")\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    items = []\n    posicion = 1  \n    tamanio_pagina=100\n\n    while True:\n        query = (\n            \"SELECT * FROM Customer \"\n            f\"WHERE Metadata.LastUpdatedTime >= '{fecha_inicio}' \"\n            f\"AND Metadata.LastUpdatedTime <= '{fecha_fin}' \"\n            f\"STARTPOSITION {posicion} \"\n            f\"MAXRESULTS {tamanio_pagina}\"\n        )\n\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/text\",\n        }\n        params = {\n            \"minorversion\": minor_version\n            }\n\n        intentos = 0\n        while intentos < max_retries:\n            try:\n                print(f\"Request API: {fecha_inicio} \u2192 {fecha_fin}, posici\u00f3n={posicion}, intento {intentos+1}\")\n                resp = requests.get(url, headers=headers, params={**params, \"query\": query}, timeout=60)\n\n                if resp.status_code == 401:  # token expirado\n                    print(\"Access token expirado, refrescando...\")\n                    access_token = _refresh_access_token()\n                    headers[\"Authorization\"] = f\"Bearer {access_token}\"\n                    intentos += 1\n                    continue\n\n                resp.raise_for_status()\n                data = resp.json()\n                lote = data.get(\"QueryResponse\", {}).get(\"Customer\", [])\n\n                print(f\"Posici\u00f3n {posicion} \u2192 {len(lote)} items\")\n                items.extend(lote)\n\n                request_payload = {\n                    \"url\": url,\n                    \"headers\": {k: v for k, v in headers.items() if k != \"Authorization\"},\n                    \"params\": params,\n                    \"query\": query,\n                }\n\n\n                if len(lote) < tamanio_pagina:\n                    return items, query, tamanio_pagina, request_payload, posicion\n                else:\n                    posicion += tamanio_pagina  # avanzar a la siguiente p\u00e1gina\n                    break\n\n            except requests.exceptions.RequestException as e:\n                intentos += 1\n                espera = 2 ** intentos\n                print(f\"Error en request ({e}), reintento {intentos}/{max_retries} en {espera}s...\")\n                time.sleep(espera)\n        else:\n            raise Exception(f\"Fall\u00f3 la request despu\u00e9s de {max_retries} intentos\")\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = _refresh_access_token() #siempre refrescamos\n    minor_version = 75\n    base_url = \"https://sandbox-quickbooks.api.intuit.com\"\n\n    start_date_str = kwargs.get(\"start_date\")\n    end_date_str = kwargs.get(\"end_date\")\n    fecha_inicio = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n    fecha_fin = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n\n    todos_items = []\n    chunk = 1\n    cursor = fecha_inicio\n\n    while cursor <= fecha_fin:\n        semana_inicio = cursor\n        semana_fin = min(cursor + timedelta(days=6), fecha_fin)\n\n        fi = semana_inicio.strftime(\"%Y-%m-%dT00:00:00Z\")\n        ff = semana_fin.strftime(\"%Y-%m-%dT23:59:59Z\")\n\n        print(f\"Chunk {chunk}: {fi} \u2192 {ff}\")\n\n        items, query, tamanio_pagina, request_payload, posicion = _fetch_qb_data(\n            realm_id, access_token, base_url, minor_version, fi, ff\n        )\n\n        ingested_at = kwargs.get(\"execution_date\")\n\n        for f in items:\n            todos_items.append({\n                \"id\": f.get(\"Id\"),\n                \"payload\": f,\n                \"ingested_at_utc\": ingested_at,\n                \"extract_window_start_utc\": fi,\n                \"extract_window_end_utc\": ff,\n                \"page_number\": posicion,\n                \"page_size\": tamanio_pagina,\n                \"request_payload\": request_payload,\n            })\n\n        print(f\"Chunk {chunk} devolvi\u00f3 {len(items)} items\")\n        chunk += 1\n        cursor = semana_fin + timedelta(days=1)\n\n    print(f\"Total items: {len(todos_items)}\")\n\n    return {\"items\": todos_items}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/ingest_qb_customers.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_customers"}, "data_loaders/ingest_qb_items.py:data_loader:python:ingest qb items": {"content": "import time\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef _refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_secret_id')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n    }\n\n    respuesta = requests.post(url, headers=headers, data=data, auth=(client_id, client_secret))\n    respuesta.raise_for_status()\n    tokens = respuesta.json()\n    print(\"Nuevo access_token obtenido v\u00eda refresh_token\")\n    return tokens[\"access_token\"]\n\n\ndef _fetch_qb_data(realm_id, access_token, base_url, minor_version,\n                   fecha_inicio, fecha_fin, max_retries=5):\n    if not realm_id or not access_token:\n        raise ValueError(\"Se requieren realm_id y access_token\")\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere base_url y minor_version\")\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Se requieren fecha_inicio y fecha_fin\")\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    items = []\n    posicion = 1  \n    tamanio_pagina=100\n\n    while True:\n        query = (\n            \"SELECT * FROM Item \"\n            f\"WHERE Metadata.LastUpdatedTime >= '{fecha_inicio}' \"\n            f\"AND Metadata.LastUpdatedTime <= '{fecha_fin}' \"\n            f\"STARTPOSITION {posicion} \"\n            f\"MAXRESULTS {tamanio_pagina}\"\n        )\n\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/text\",\n        }\n        params = {\n            \"minorversion\": minor_version\n            }\n\n        intentos = 0\n        while intentos < max_retries:\n            try:\n                print(f\"Request API: {fecha_inicio} \u2192 {fecha_fin}, posici\u00f3n={posicion}, intento {intentos+1}\")\n                resp = requests.get(url, headers=headers, params={**params, \"query\": query}, timeout=60)\n\n                if resp.status_code == 401:  # token expirado\n                    print(\"Access token expirado, refrescando...\")\n                    access_token = _refresh_access_token()\n                    headers[\"Authorization\"] = f\"Bearer {access_token}\"\n                    intentos += 1\n                    continue\n\n                resp.raise_for_status()\n                data = resp.json()\n                lote = data.get(\"QueryResponse\", {}).get(\"Item\", [])\n\n                print(f\"Posici\u00f3n {posicion} \u2192 {len(lote)} items\")\n                items.extend(lote)\n\n                request_payload = {\n                    \"url\": url,\n                    \"headers\": {k: v for k, v in headers.items() if k != \"Authorization\"},\n                    \"params\": params,\n                    \"query\": query,\n                }\n\n\n                if len(lote) < tamanio_pagina:\n                    return items, query, tamanio_pagina, request_payload, posicion\n                else:\n                    posicion += tamanio_pagina  # avanzar a la siguiente p\u00e1gina\n                    break\n\n            except requests.exceptions.RequestException as e:\n                intentos += 1\n                espera = 2 ** intentos\n                print(f\"Error en request ({e}), reintento {intentos}/{max_retries} en {espera}s...\")\n                time.sleep(espera)\n        else:\n            raise Exception(f\"Fall\u00f3 la request despu\u00e9s de {max_retries} intentos\")\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = _refresh_access_token() #siempre refrescamos\n    minor_version = 75\n    base_url = \"https://sandbox-quickbooks.api.intuit.com\"\n\n    start_date_str = kwargs.get(\"start_date\")\n    end_date_str = kwargs.get(\"end_date\")\n    fecha_inicio = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n    fecha_fin = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n\n    todos_items = []\n    chunk = 1\n    cursor = fecha_inicio\n\n    while cursor <= fecha_fin:\n        semana_inicio = cursor\n        semana_fin = min(cursor + timedelta(days=6), fecha_fin)\n\n        fi = semana_inicio.strftime(\"%Y-%m-%dT00:00:00Z\")\n        ff = semana_fin.strftime(\"%Y-%m-%dT23:59:59Z\")\n\n        print(f\"Chunk {chunk}: {fi} \u2192 {ff}\")\n\n        items, query, tamanio_pagina, request_payload, posicion = _fetch_qb_data(\n            realm_id, access_token, base_url, minor_version, fi, ff\n        )\n\n        ingested_at = kwargs.get(\"execution_date\")\n\n        for f in items:\n            todos_items.append({\n                \"id\": f.get(\"Id\"),\n                \"payload\": f,\n                \"ingested_at_utc\": ingested_at,\n                \"extract_window_start_utc\": fi,\n                \"extract_window_end_utc\": ff,\n                \"page_number\": posicion,\n                \"page_size\": tamanio_pagina,\n                \"request_payload\": request_payload,\n            })\n\n        print(f\"Chunk {chunk} devolvi\u00f3 {len(items)} items\")\n        chunk += 1\n        cursor = semana_fin + timedelta(days=1)\n\n    print(f\"Total items: {len(todos_items)}\")\n\n    return {\"items\": todos_items}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest_qb_items.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_items"}, "data_loaders/loader.py:data_loader:python:loader": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/loader.py", "language": "python", "type": "data_loader", "uuid": "loader"}, "data_loaders/cheerful_explorer.py:data_loader:python:cheerful explorer": {"content": "import time\nimport requests\nimport logging\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nlogger = logging.getLogger(__name__)\n\n\ndef _refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n    }\n\n    resp = requests.post(url, headers=headers, data=data, auth=(client_id, client_secret))\n    resp.raise_for_status()\n    tokens = resp.json()\n    logger.info(\"Nuevo access_token obtenido v\u00eda refresh_token\")\n    return tokens[\"access_token\"]\n\n\ndef _fetch_qb_data(realm_id, access_token, base_url, minor_version,\n                   fecha_inicio, fecha_fin, tamanio_pagina=100, max_retries=5):\n    \"\"\"\n    Devuelve la respuesta cruda de QuickBooks pero acumulando todas las p\u00e1ginas.\n    \"\"\"\n    if not realm_id or not access_token:\n        raise ValueError(\"Se requieren realm_id y access_token\")\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere base_url y minor_version\")\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Se requieren fecha_inicio y fecha_fin\")\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    pagina = 1  \n    todas_facturas = []\n\n    while True:\n        query = (\n            \"SELECT * FROM Invoice \"\n            f\"WHERE Metadata.LastUpdatedTime >= '{fecha_inicio}' \"\n            f\"AND Metadata.LastUpdatedTime <= '{fecha_fin}' \"\n            f\"STARTPOSITION {pagina} \"\n            f\"MAXRESULTS {tamanio_pagina}\"\n        )\n\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/text\",\n        }\n        params = {\"minorversion\": minor_version}\n\n        intentos = 0\n        while intentos < max_retries:\n            try:\n                logger.info(f\"Request a la API: p\u00e1gina={pagina}, intento {intentos+1}\")\n                response = requests.get(url, headers=headers, params={**params, \"query\": query}, timeout=60)\n\n                if response.status_code == 401:  \n                    logger.warning(\"Access token expirado, refrescando...\")\n                    access_token = _refresh_access_token()\n                    headers[\"Authorization\"] = f\"Bearer {access_token}\"\n                    intentos += 1\n                    continue\n\n                response.raise_for_status()\n                data = response.json()\n\n                lote = data.get(\"QueryResponse\", {}).get(\"Invoice\", [])\n                todas_facturas.extend(lote)\n\n                # si hay menos resultados que el tama\u00f1o de p\u00e1gina \u2192 ya no hay m\u00e1s p\u00e1ginas\n                if len(lote) < tamanio_pagina:\n                    return {\n                        \"QueryResponse\": {\"Invoice\": todas_facturas},\n                        \"time\": data.get(\"time\"),\n                    }\n                else:\n                    pagina += tamanio_pagina\n                    break  # sigue a la siguiente p\u00e1gina\n\n            except requests.exceptions.RequestException as e:\n                intentos += 1\n                espera = 2 ** intentos\n                logger.warning(f\"Error en request ({e}), reintento {intentos}/{max_retries} en {espera}s...\")\n                time.sleep(espera)\n\n        else:\n            raise Exception(f\"Fall\u00f3 la request despu\u00e9s de {max_retries} intentos\")\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Loader para traer facturas de QuickBooks (respuesta cruda, todas las p\u00e1ginas).\n    \"\"\"\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = get_secret_value('qb_access_token')\n    minor_version = 75\n    base_url = \"https://sandbox-quickbooks.api.intuit.com\"\n\n    fecha_inicio = kwargs.get(\"fecha_inicio\") or \"2025-07-01T00:00:00-07:00\"\n    fecha_fin = kwargs.get(\"fecha_fin\") or \"2025-09-09T23:59:59-07:00\"\n\n    data = _fetch_qb_data(\n        realm_id, access_token, base_url, minor_version, fecha_inicio, fecha_fin\n    )\n    logger.info(f\"Loader devolvi\u00f3 {len(data.get('QueryResponse', {}).get('Invoice', []))} facturas\")\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test b\u00e1sico para validar que el loader devuelve algo.\n    \"\"\"\n    assert output is not None, 'El output est\u00e1 vac\u00edo'\n    assert isinstance(output, dict), 'El output debe ser un dict (respuesta JSON cruda)'\n    assert \"QueryResponse\" in output, 'Falta la clave QueryResponse en la respuesta'\n\n", "file_path": "data_loaders/cheerful_explorer.py", "language": "python", "type": "data_loader", "uuid": "cheerful_explorer"}, "data_loaders/wild_cloud.py:data_loader:python:wild cloud": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/wild_cloud.py", "language": "python", "type": "data_loader", "uuid": "wild_cloud"}, "transformers/transform_qb_items.py:transformer:python:transform qb items": {"content": "import pandas as pd\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n\n    if not data:\n        return pd.DataFrame()\n\n    items = data.get(\"items\", data)\n\n    df = pd.DataFrame(items)\n\n    return df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_qb_items.py", "language": "python", "type": "transformer", "uuid": "transform_qb_items"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/transform_qb_customers.py:transformer:python:transform qb customers": {"content": "import pandas as pd\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n\n    if not data:\n        return pd.DataFrame()\n\n    items = data.get(\"items\", data)\n\n    df = pd.DataFrame(items)\n\n    return df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_qb_customers.py", "language": "python", "type": "transformer", "uuid": "transform_qb_customers"}, "transformers/transform_qb_invoices.py:transformer:python:transform qb invoices": {"content": "import pandas as pd\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n\n    if not data:\n        return pd.DataFrame()\n\n    facturas = data.get(\"invoices\", data)\n\n    df = pd.DataFrame(facturas)\n\n    return df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_qb_invoices.py", "language": "python", "type": "transformer", "uuid": "transform_qb_invoices"}, "pipelines/qb_customers_backfill/metadata.yaml:pipeline:yaml:qb customers backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/ingest_qb_customers.py\n    file_source:\n      path: data_loaders/ingest_qb_customers.py\n  downstream_blocks:\n  - transform_qb_customers\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_qb_customers\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_qb_customers\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: transformers/transform_qb_customers.py\n    file_source:\n      path: transformers/transform_qb_customers.py\n  downstream_blocks:\n  - extractor_qb_customers\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transform_qb_customers\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - ingest_qb_customers\n  uuid: transform_qb_customers\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_exporters/extractor_qb_customers.py\n    file_source:\n      path: data_exporters/extractor_qb_customers.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extractor_qb_customers\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - transform_qb_customers\n  uuid: extractor_qb_customers\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-10 02:58:31.027882+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_customers_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_customers_backfill\nvariables:\n  end_date: '2025-08-31'\n  start_date: '2025-07-01'\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_customers_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_customers_backfill/metadata"}, "pipelines/qb_customers_backfill/__init__.py:pipeline:python:qb customers backfill/  init  ": {"content": "", "file_path": "pipelines/qb_customers_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_customers_backfill/__init__"}, "pipelines/qb_invoices_backfill/metadata.yaml:pipeline:yaml:qb invoices backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - transform_qb_invoices\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_qb_invoices\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_qb_invoices\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - extractor_qb_invoices\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transform_qb_invoices\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - ingest_qb_invoices\n  uuid: transform_qb_invoices\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extractor_qb_invoices\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - transform_qb_invoices\n  uuid: extractor_qb_invoices\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-10 02:58:31.027882+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_invoices_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_invoices_backfill\nvariables:\n  end_date: '2025-08-31'\n  start_date: '2025-07-01'\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_invoices_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/metadata"}, "pipelines/qb_invoices_backfill/triggers.yaml:pipeline:yaml:qb invoices backfill/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2025-09-11 02:54:29.927005\n  name: qb_invoice_trigger\n  pipeline_uuid: qb_invoices_backfill\n  schedule_interval: '@once'\n  schedule_type: time\n  settings: null\n  sla: null\n  start_time: 2025-09-15 04:05:00\n  status: inactive\n  token: 077634bf147e40a989b0cfc614ecad93\n  variables:\n    end_date: '2025-08-31'\n    start_date: '2025-07-01'\n", "file_path": "pipelines/qb_invoices_backfill/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/triggers"}, "pipelines/qb_invoices_backfill/__init__.py:pipeline:python:qb invoices backfill/  init  ": {"content": "", "file_path": "pipelines/qb_invoices_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_invoices_backfill/__init__"}, "pipelines/qb_items_backfill/metadata.yaml:pipeline:yaml:qb items backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/ingest_qb_items.py\n    file_source:\n      path: data_loaders/ingest_qb_items.py\n  downstream_blocks:\n  - transform_qb_items\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_qb_items\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_qb_items\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: transformers/transform_qb_items.py\n    file_source:\n      path: transformers/transform_qb_items.py\n  downstream_blocks:\n  - extractor_qb_items\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transform_qb_items\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - ingest_qb_items\n  uuid: transform_qb_items\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_exporters/extractor_qb_items.py\n    file_source:\n      path: data_exporters/extractor_qb_items.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extractor_qb_items\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - transform_qb_items\n  uuid: extractor_qb_items\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-11 04:23:12.122542+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_items_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_items_backfill\nvariables:\n  end_date: '2025-08-31'\n  start_date: '2025-07-01'\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_items_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_items_backfill/metadata"}, "pipelines/qb_items_backfill/__init__.py:pipeline:python:qb items backfill/  init  ": {"content": "", "file_path": "pipelines/qb_items_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_items_backfill/__init__"}, "/home/src/scheduler/data_loaders/ingest_qb_invoice.py:data_loader:python:home/src/scheduler/data loaders/ingest qb invoice": {"content": "import time\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport requests\nimport base64\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef _refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_secret_id')  \n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    creds = f\"{client_id}:{client_secret}\"\n    encoded = base64.b64encode(creds.encode()).decode()\n\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"Authorization\": f\"Basic {encoded}\",\n    }\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n    }\n\n    resp = requests.post(url, headers=headers, data=data, timeout=30)\n    resp.raise_for_status()\n    tokens = resp.json()\n    print(\"Nuevo access_token obtenido v\u00eda refresh_token\")\n    return tokens[\"access_token\"]\n\n\n\ndef _fetch_qb_data(realm_id, access_token, base_url, minor_version,\n                   fecha_inicio, fecha_fin, max_retries=5):\n    if not realm_id or not access_token:\n        raise ValueError(\"Se requieren realm_id y access_token\")\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere base_url y minor_version\")\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Se requieren fecha_inicio y fecha_fin\")\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    invoices = []\n    posicion = 1  \n    tamanio_pagina=100\n    page_number = 1\n\n    while True:\n        query = (\n            \"SELECT * FROM Invoice \"\n            f\"WHERE Metadata.LastUpdatedTime >= '{fecha_inicio}' \"\n            f\"AND Metadata.LastUpdatedTime <= '{fecha_fin}' \"\n            f\"STARTPOSITION {posicion} \"\n            f\"MAXRESULTS {tamanio_pagina}\"\n        )\n\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/text\",\n        }\n        params = {\n            \"minorversion\": minor_version\n            }\n\n        intentos = 0\n        while intentos < max_retries:\n            try:\n                print(f\"Request API: {fecha_inicio} \u2192 {fecha_fin}, posici\u00f3n={posicion}, intento {intentos+1}\")\n                resp = requests.get(url, headers=headers, params={**params, \"query\": query}, timeout=60)\n\n                if resp.status_code == 401:  # token expirado\n                    print(\"Access token expirado, refrescando...\")\n                    access_token= _refresh_access_token()\n                    headers[\"Authorization\"] = f\"Bearer {access_token}\"\n                    intentos += 1\n                    continue\n\n                resp.raise_for_status()\n                data = resp.json()\n                lote = data.get(\"QueryResponse\", {}).get(\"Invoice\", [])\n\n                print(f\"Posici\u00f3n {posicion} \u2192 {len(lote)} invoices\")\n                invoices.extend(lote)\n\n                request_payload = {\n                    \"url\": url,\n                    \"headers\": {k: v for k, v in headers.items() if k != \"Authorization\"},\n                    \"params\": params,\n                    \"query\": query,\n                }\n\n\n                if len(lote) < tamanio_pagina:\n                    return invoices, query, tamanio_pagina, request_payload, posicion\n                else:\n                    posicion += tamanio_pagina  # avanzar a la siguiente p\u00e1gina\n                    page_number += 1\n                    break\n\n            except requests.exceptions.RequestException as e:\n                intentos += 1\n                espera = 2 ** intentos\n                print(f\"Error en request ({e}), reintento {intentos}/{max_retries} en {espera}s...\")\n                time.sleep(espera)\n        else:\n            raise Exception(f\"Fall\u00f3 la request despu\u00e9s de {max_retries} intentos\")\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    realm_id = get_secret_value('qb_realm_id')\n    access_token= _refresh_access_token() #siempre refrescamos\n    minor_version = 75\n    base_url = \"https://sandbox-quickbooks.api.intuit.com\"\n\n    start_date_str = kwargs.get(\"start_date\")\n    end_date_str = kwargs.get(\"end_date\")\n    fecha_inicio = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n    fecha_fin = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n\n    todos_invoices = []\n    chunk = 1\n    cursor = fecha_inicio\n\n    while cursor <= fecha_fin:\n        semana_inicio = cursor\n        semana_fin = min(cursor + timedelta(days=6), fecha_fin)\n\n        fi = semana_inicio.strftime(\"%Y-%m-%dT00:00:00Z\")\n        ff = semana_fin.strftime(\"%Y-%m-%dT23:59:59Z\")\n\n        print(f\"Chunk {chunk}: {fi} \u2192 {ff}\")\n\n        invoices, query, tamanio_pagina, request_payload, page_number = _fetch_qb_data(\n            realm_id, access_token, base_url, minor_version, fi, ff\n        )\n\n        ingested_at = kwargs.get(\"execution_date\")\n\n        for f in invoices:\n            todos_invoices.append({\n                \"id\": f.get(\"Id\"),\n                \"payload\": f,\n                \"ingested_at_utc\": ingested_at,\n                \"extract_window_start_utc\": fi,\n                \"extract_window_end_utc\": ff,\n                \"page_number\": page_number,\n                \"page_size\": tamanio_pagina,\n                \"request_payload\": request_payload,\n            })\n\n        print(f\"Chunk {chunk} devolvi\u00f3 {len(invoices)} invoices\")\n        chunk += 1\n        cursor = semana_fin + timedelta(days=1)\n\n    print(f\"Total invoices: {len(todos_invoices)}\")\n\n    return {\"invoices\": todos_invoices}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "/home/src/scheduler/data_loaders/ingest_qb_invoice.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_invoice"}, "/home/src/scheduler/transformers/transform_qb_invoices.py:transformer:python:home/src/scheduler/transformers/transform qb invoices": {"content": "import pandas as pd\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n\n    if not data:\n        return pd.DataFrame()\n\n    facturas = data.get(\"invoices\", data)\n\n    df = pd.DataFrame(facturas)\n\n    return df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/scheduler/transformers/transform_qb_invoices.py", "language": "python", "type": "transformer", "uuid": "transform_qb_invoices"}, "/home/src/scheduler/data_loaders/ingest_qb_customers.py:data_loader:python:home/src/scheduler/data loaders/ingest qb customers": {"content": "import time\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport requests\nimport base64\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef _refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_secret_id')  \n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    creds = f\"{client_id}:{client_secret}\"\n    encoded = base64.b64encode(creds.encode()).decode()\n\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"Authorization\": f\"Basic {encoded}\",\n    }\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n    }\n\n    resp = requests.post(url, headers=headers, data=data, timeout=30)\n    resp.raise_for_status()\n    tokens = resp.json()\n    print(\"Nuevo access_token obtenido v\u00eda refresh_token\")\n    return tokens[\"access_token\"]\n\n\ndef _fetch_qb_data(realm_id, access_token, base_url, minor_version,\n                   fecha_inicio, fecha_fin, max_retries=5):\n    if not realm_id or not access_token:\n        raise ValueError(\"Se requieren realm_id y access_token\")\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere base_url y minor_version\")\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Se requieren fecha_inicio y fecha_fin\")\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    customers = []\n    posicion = 1  \n    tamanio_pagina = 100\n    page_number = 1\n\n    while True:\n        query = (\n            \"SELECT * FROM Customer \"\n            f\"WHERE Metadata.LastUpdatedTime >= '{fecha_inicio}' \"\n            f\"AND Metadata.LastUpdatedTime <= '{fecha_fin}' \"\n            f\"STARTPOSITION {posicion} \"\n            f\"MAXRESULTS {tamanio_pagina}\"\n        )\n\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/text\",\n        }\n        params = {\n            \"minorversion\": minor_version\n        }\n\n        intentos = 0\n        while intentos < max_retries:\n            try:\n                print(f\"Request API: {fecha_inicio} \u2192 {fecha_fin}, posici\u00f3n={posicion}, intento {intentos+1}\")\n                resp = requests.get(url, headers=headers, params={**params, \"query\": query}, timeout=60)\n\n                if resp.status_code == 401:  # token expirado\n                    print(\"Access token expirado, refrescando...\")\n                    access_token = _refresh_access_token()\n                    headers[\"Authorization\"] = f\"Bearer {access_token}\"\n                    intentos += 1\n                    continue\n\n                resp.raise_for_status()\n                data = resp.json()\n                lote = data.get(\"QueryResponse\", {}).get(\"Customer\", [])\n\n                print(f\"Posici\u00f3n {posicion} \u2192 {len(lote)} customers\")\n                customers.extend(lote)\n\n                request_payload = {\n                    \"url\": url,\n                    \"headers\": {k: v for k, v in headers.items() if k != \"Authorization\"},\n                    \"params\": params,\n                    \"query\": query,\n                }\n\n                if len(lote) < tamanio_pagina:\n                    return customers, query, tamanio_pagina, request_payload, posicion\n                else:\n                    posicion += tamanio_pagina  # avanzar a la siguiente p\u00e1gina\n                    page_number += 1\n                    break\n\n            except requests.exceptions.RequestException as e:\n                intentos += 1\n                espera = 2 ** intentos\n                print(f\"Error en request ({e}), reintento {intentos}/{max_retries} en {espera}s...\")\n                time.sleep(espera)\n        else:\n            raise Exception(f\"Fall\u00f3 la request despu\u00e9s de {max_retries} intentos\")\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = _refresh_access_token()  # siempre refrescamos\n    minor_version = 75\n    base_url = \"https://sandbox-quickbooks.api.intuit.com\"\n\n    start_date_str = kwargs.get(\"start_date\")\n    end_date_str = kwargs.get(\"end_date\")\n    fecha_inicio = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n    fecha_fin = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n\n    todos_customers = []\n    chunk = 1\n    cursor = fecha_inicio\n\n    while cursor <= fecha_fin:\n        semana_inicio = cursor\n        semana_fin = min(cursor + timedelta(days=6), fecha_fin)\n\n        fi = semana_inicio.strftime(\"%Y-%m-%dT00:00:00Z\")\n        ff = semana_fin.strftime(\"%Y-%m-%dT23:59:59Z\")\n\n        print(f\"Chunk {chunk}: {fi} \u2192 {ff}\")\n\n        customers, query, tamanio_pagina, request_payload, page_number = _fetch_qb_data(\n            realm_id, access_token, base_url, minor_version, fi, ff\n        )\n\n        ingested_at = kwargs.get(\"execution_date\")\n\n        for f in customers:\n            todos_customers.append({\n                \"id\": f.get(\"Id\"),\n                \"payload\": f,\n                \"ingested_at_utc\": ingested_at,\n                \"extract_window_start_utc\": fi,\n                \"extract_window_end_utc\": ff,\n                \"page_number\": page_number,\n                \"page_size\": tamanio_pagina,\n                \"request_payload\": request_payload,\n            })\n\n        print(f\"Chunk {chunk} devolvi\u00f3 {len(customers)} customers\")\n        chunk += 1\n        cursor = semana_fin + timedelta(days=1)\n\n    print(f\"Total customers: {len(todos_customers)}\")\n\n    return {\"customers\": todos_customers}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/scheduler/data_loaders/ingest_qb_customers.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_customers"}, "/home/src/scheduler/data_loaders/ingest_qb_items.py:data_loader:python:home/src/scheduler/data loaders/ingest qb items": {"content": "import time\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport requests\nimport base64\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef _refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_secret_id')  \n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    creds = f\"{client_id}:{client_secret}\"\n    encoded = base64.b64encode(creds.encode()).decode()\n\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"Authorization\": f\"Basic {encoded}\",\n    }\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n    }\n\n    resp = requests.post(url, headers=headers, data=data, timeout=30)\n    resp.raise_for_status()\n    tokens = resp.json()\n    print(\"Nuevo access_token obtenido v\u00eda refresh_token\")\n    return tokens[\"access_token\"]\n\n\ndef _fetch_qb_data(realm_id, access_token, base_url, minor_version,\n                   fecha_inicio, fecha_fin, max_retries=5):\n    if not realm_id or not access_token:\n        raise ValueError(\"Se requieren realm_id y access_token\")\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere base_url y minor_version\")\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Se requieren fecha_inicio y fecha_fin\")\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    items = []\n    posicion = 1  \n    tamanio_pagina=100\n    page_number = 1\n\n    while True:\n        query = (\n            \"SELECT * FROM Item \"\n            f\"WHERE Metadata.LastUpdatedTime >= '{fecha_inicio}' \"\n            f\"AND Metadata.LastUpdatedTime <= '{fecha_fin}' \"\n            f\"STARTPOSITION {posicion} \"\n            f\"MAXRESULTS {tamanio_pagina}\"\n        )\n\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/text\",\n        }\n        params = {\n            \"minorversion\": minor_version\n            }\n\n        intentos = 0\n        while intentos < max_retries:\n            try:\n                print(f\"Request API: {fecha_inicio} \u2192 {fecha_fin}, posici\u00f3n={posicion}, intento {intentos+1}\")\n                resp = requests.get(url, headers=headers, params={**params, \"query\": query}, timeout=60)\n\n                if resp.status_code == 401:  # token expirado\n                    print(\"Access token expirado, refrescando...\")\n                    access_token = _refresh_access_token()\n                    headers[\"Authorization\"] = f\"Bearer {access_token}\"\n                    intentos += 1\n                    continue\n\n                resp.raise_for_status()\n                data = resp.json()\n                lote = data.get(\"QueryResponse\", {}).get(\"Item\", [])\n\n                print(f\"Posici\u00f3n {posicion} \u2192 {len(lote)} items\")\n                items.extend(lote)\n\n                request_payload = {\n                    \"url\": url,\n                    \"headers\": {k: v for k, v in headers.items() if k != \"Authorization\"},\n                    \"params\": params,\n                    \"query\": query,\n                }\n\n\n                if len(lote) < tamanio_pagina:\n                    return items, query, tamanio_pagina, request_payload, posicion\n                else:\n                    posicion += tamanio_pagina  # avanzar a la siguiente p\u00e1gina\n                    page_number +=1\n                    break\n\n            except requests.exceptions.RequestException as e:\n                intentos += 1\n                espera = 2 ** intentos\n                print(f\"Error en request ({e}), reintento {intentos}/{max_retries} en {espera}s...\")\n                time.sleep(espera)\n        else:\n            raise Exception(f\"Fall\u00f3 la request despu\u00e9s de {max_retries} intentos\")\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = _refresh_access_token() #siempre refrescamos\n    minor_version = 75\n    base_url = \"https://sandbox-quickbooks.api.intuit.com\"\n\n    start_date_str = kwargs.get(\"start_date\")\n    end_date_str = kwargs.get(\"end_date\")\n    fecha_inicio = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n    fecha_fin = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n\n    todos_items = []\n    chunk = 1\n    cursor = fecha_inicio\n\n    while cursor <= fecha_fin:\n        semana_inicio = cursor\n        semana_fin = min(cursor + timedelta(days=6), fecha_fin)\n\n        fi = semana_inicio.strftime(\"%Y-%m-%dT00:00:00Z\")\n        ff = semana_fin.strftime(\"%Y-%m-%dT23:59:59Z\")\n\n        print(f\"Chunk {chunk}: {fi} \u2192 {ff}\")\n\n        items, query, tamanio_pagina, request_payload, page_number = _fetch_qb_data(\n            realm_id, access_token, base_url, minor_version, fi, ff\n        )\n\n        ingested_at = kwargs.get(\"execution_date\")\n\n        for f in items:\n            todos_items.append({\n                \"id\": f.get(\"Id\"),\n                \"payload\": f,\n                \"ingested_at_utc\": ingested_at,\n                \"extract_window_start_utc\": fi,\n                \"extract_window_end_utc\": ff,\n                \"page_number\": page_number,\n                \"page_size\": tamanio_pagina,\n                \"request_payload\": request_payload,\n            })\n\n        print(f\"Chunk {chunk} devolvi\u00f3 {len(items)} items\")\n        chunk += 1\n        cursor = semana_fin + timedelta(days=1)\n\n    print(f\"Total items: {len(todos_items)}\")\n\n    return {\"items\": todos_items}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/scheduler/data_loaders/ingest_qb_items.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_items"}, "/home/src/scheduler/data_exporters/extractor_qb_customers.py:data_exporter:python:home/src/scheduler/data exporters/extractor qb customers": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom datetime import datetime\nfrom os import path\nimport psycopg2\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport json\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    if df is None or df.empty:\n        print(\"No hay customers para exportar.\")\n        return\n\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    dbname = get_secret_value('pg_db')\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n\n    schema = 'raw'\n    table = 'qb_customers'\n    report_table = 'backfill_report_customers'\n    pipeline_name = \"qb_customers_backfill\"\n\n    # Serializar payloads\n    df = df.copy()\n    if \"payload\" in df.columns:\n        df[\"payload\"] = df[\"payload\"].apply(\n            lambda x: json.dumps(x) if isinstance(x, (dict, list)) else str(x)\n        )\n    if \"request_payload\" in df.columns:\n        df[\"request_payload\"] = df[\"request_payload\"].apply(\n            lambda x: json.dumps(x) if isinstance(x, (dict, list)) else str(x)\n        )\n\n    started_at = datetime.utcnow()\n    print(f\"Exportando {len(df)} customers a {schema}.{table} en {host}:{port}/{dbname}\")\n\n    conn = psycopg2.connect(\n        host=host,\n        port=port,\n        dbname=dbname,\n        user=user,\n        password=password\n    )\n    conn.autocommit = False\n\n    try:\n        with conn.cursor() as cur:\n            # Crear esquema si no existe\n            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n\n            # Crear tabla destino\n            ddl = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema}.{table} (\n                id TEXT PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP,\n                extract_window_start_utc TIMESTAMP,\n                extract_window_end_utc TIMESTAMP,\n                page_number INT,\n                page_size INT,\n                request_payload TEXT\n            );\n            \"\"\"\n            cur.execute(ddl)\n\n            # Insert con UPSERT\n            insert_sql = f\"\"\"\n            INSERT INTO {schema}.{table} (\n                id, payload, ingested_at_utc,\n                extract_window_start_utc, extract_window_end_utc,\n                page_number, page_size, request_payload\n            )\n            VALUES (\n                %(id)s, %(payload)s, %(ingested_at_utc)s,\n                %(extract_window_start_utc)s, %(extract_window_end_utc)s,\n                %(page_number)s, %(page_size)s, %(request_payload)s\n            )\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number,\n                page_size = EXCLUDED.page_size,\n                request_payload = EXCLUDED.request_payload;\n            \"\"\"\n            records = df.to_dict(orient='records')\n            for row in records:\n                cur.execute(insert_sql, row)\n\n            ddl_report = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema}.{report_table} (\n                id SERIAL PRIMARY KEY,\n                pipeline_name TEXT,\n                entity TEXT,\n                row_count INT,\n                window_start TIMESTAMP,\n                window_end TIMESTAMP,\n                started_at TIMESTAMP,\n                ended_at TIMESTAMP,\n                status TEXT,\n                error_message TEXT\n            );\n            \"\"\"\n            cur.execute(ddl_report)\n\n            ended_at = datetime.utcnow()\n            cur.execute(\n                f\"\"\"\n                INSERT INTO {schema}.{report_table} \n                (pipeline_name, entity, row_count, window_start, window_end, \n                 started_at, ended_at, status, error_message)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n                \"\"\",\n                (\n                    pipeline_name,\n                    \"customers\",\n                    len(df),\n                    df[\"extract_window_start_utc\"].min(),\n                    df[\"extract_window_end_utc\"].max(),\n                    started_at,\n                    ended_at,\n                    \"success\",\n                    None,\n                )\n            )\n\n        conn.commit()\n        print(f\"Exportaci\u00f3n completada: {len(df)} customers procesados.\")\n\n    except Exception as e:\n        conn.rollback()\n        ended_at = datetime.utcnow()\n        with conn.cursor() as cur:\n            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n            cur.execute(f\"\"\"\n                CREATE TABLE IF NOT EXISTS {schema}.{report_table} (\n                    id SERIAL PRIMARY KEY,\n                    pipeline_name TEXT,\n                    entity TEXT,\n                    row_count INT,\n                    window_start TIMESTAMP,\n                    window_end TIMESTAMP,\n                    started_at TIMESTAMP,\n                    ended_at TIMESTAMP,\n                    status TEXT,\n                    error_message TEXT\n                );\n            \"\"\")\n            cur.execute(\n                f\"\"\"\n                INSERT INTO {schema}.{report_table} \n                (pipeline_name, entity, row_count, window_start, window_end, \n                 started_at, ended_at, status, error_message)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n                \"\"\",\n                (\n                    pipeline_name,\n                    \"customers\",\n                    0,\n                    None,\n                    None,\n                    started_at,\n                    ended_at,\n                    \"error\",\n                    str(e),\n                )\n            )\n        conn.commit()\n        print(f\"Error al exportar customers: {e}\")\n        raise e\n    finally:\n        conn.close()", "file_path": "/home/src/scheduler/data_exporters/extractor_qb_customers.py", "language": "python", "type": "data_exporter", "uuid": "extractor_qb_customers"}, "/home/src/scheduler/transformers/transform_qb_customers.py:transformer:python:home/src/scheduler/transformers/transform qb customers": {"content": "import pandas as pd\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n\n    if not data:\n        return pd.DataFrame()\n\n    customers = data.get(\"customers\", data)\n    df = pd.DataFrame(customers)\n\n    return df\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/scheduler/transformers/transform_qb_customers.py", "language": "python", "type": "transformer", "uuid": "transform_qb_customers"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}